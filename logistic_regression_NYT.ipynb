{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c035ug0.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c039a2t.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c03ad2h.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c03cnx9.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c03d5p6.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c03de8j.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c03deaq.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c03dlna.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c03g070.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c03gq7p.txt</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FILENAME  SOURCE\n",
       "0  c035ug0.txt  Reddit\n",
       "1  c039a2t.txt  Reddit\n",
       "2  c03ad2h.txt  Reddit\n",
       "3  c03cnx9.txt  Reddit\n",
       "4  c03d5p6.txt  Reddit\n",
       "5  c03de8j.txt  Reddit\n",
       "6  c03deaq.txt  Reddit\n",
       "7  c03dlna.txt  Reddit\n",
       "8  c03g070.txt  Reddit\n",
       "9  c03gq7p.txt  Reddit"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "''' \n",
    "making the initial dataframes\n",
    "(i delay concatenating them so that i can potentially reuse each with different \"versions\" of the other)\n",
    "'''\n",
    "\n",
    "# filepath to the folder containing the metadata\n",
    "corpus_path = '/Users/faithrta/engl_490_python/LESSON_9_PSET_DATA/'\n",
    "\n",
    "# making dataframes for each csv file (one for the reddit data, another for the nyt one)\n",
    "meta_reddit = pd.read_csv(corpus_path + 'REDDIT_news_2008_meta.csv', encoding='latin1')\n",
    "meta_nyt = pd.read_csv(corpus_path + 'NYT_2008_META.csv', encoding='latin1')\n",
    "\n",
    "# deleting all columns of the original csv files except for the one containing filenames\n",
    "meta_reddit = meta_reddit[['file_id']].rename(columns = {\"file_id\": \"FILENAME\"})\n",
    "meta_nyt = meta_nyt[['file_id']].rename(columns = {\"file_id\": \"FILENAME\"})\n",
    "\n",
    "# creating a new column stating the source of each text (for when they're concatenated)\n",
    "meta_reddit['SOURCE'] = 'Reddit'\n",
    "meta_nyt['SOURCE'] = 'NYT'\n",
    "\n",
    "# with the original datasets, my code never stopped running; I've thus cut down\n",
    "# the number of entries I take from reddit and the nyt, unfortunately\n",
    "meta_reddit_cut = meta_reddit.iloc[0:5000]\n",
    "meta_nyt_cut = meta_nyt.iloc[0:5000]\n",
    "\n",
    "meta_reddit_cut.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c035ug0.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>On your social networks all your friends and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c039a2t.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Clearly Angela Durante is in dire need of a cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03ad2h.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>If you enjoyed this video, you will love this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03cnx9.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Seriously doubt truthnews factors into that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03d5p6.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>I have a major issue with the work skeptic the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>NYT_2008/5/5481dcc338f0d874625c9456.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>NYT_2008/5/5481dc6a38f0d874625c944b.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>Hoping to curb the increase in the number of y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>NYT_2008/5/5481dc6538f0d874625c944a.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>BEIRUT, Lebanon — A gunman opened fire in a mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>NYT_2008/5/5481dc5d38f0d874625c9449.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PARIS — It is \\nVladimir V. Putin\\n’s first tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>NYT_2008/5/5481dc4038f0d874625c9444.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9979 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FILENAME  SOURCE  \\\n",
       "0                REDDIT_2008_text/c035ug0.txt  Reddit   \n",
       "1                REDDIT_2008_text/c039a2t.txt  Reddit   \n",
       "2                REDDIT_2008_text/c03ad2h.txt  Reddit   \n",
       "3                REDDIT_2008_text/c03cnx9.txt  Reddit   \n",
       "4                REDDIT_2008_text/c03d5p6.txt  Reddit   \n",
       "...                                       ...     ...   \n",
       "9974  NYT_2008/5/5481dcc338f0d874625c9456.txt     NYT   \n",
       "9975  NYT_2008/5/5481dc6a38f0d874625c944b.txt     NYT   \n",
       "9976  NYT_2008/5/5481dc6538f0d874625c944a.txt     NYT   \n",
       "9977  NYT_2008/5/5481dc5d38f0d874625c9449.txt     NYT   \n",
       "9978  NYT_2008/5/5481dc4038f0d874625c9444.txt     NYT   \n",
       "\n",
       "                                                   TEXT  \n",
       "0     On your social networks all your friends and c...  \n",
       "1     Clearly Angela Durante is in dire need of a cu...  \n",
       "2     If you enjoyed this video, you will love this ...  \n",
       "3         Seriously doubt truthnews factors into that.   \n",
       "4     I have a major issue with the work skeptic the...  \n",
       "...                                                 ...  \n",
       "9974  BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...  \n",
       "9975  Hoping to curb the increase in the number of y...  \n",
       "9976  BEIRUT, Lebanon — A gunman opened fire in a mo...  \n",
       "9977  PARIS — It is \\nVladimir V. Putin\\n’s first tr...  \n",
       "9978  PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...  \n",
       "\n",
       "[9979 rows x 3 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 1 contd (pt 2)\n",
    "\n",
    "import codecs\n",
    "\n",
    "''' \n",
    "adding the full text of each file as a column\n",
    "(technically this isn't needed until later but i thought it'd be good for me to see the texts\n",
    "to generate ideas for question 2)\n",
    "'''\n",
    "\n",
    "# added this because I was getting an error message for \"meta_reddit_cut['TEXT'] = ''\"\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# adding new columns for each file's full text\n",
    "meta_reddit_cut['TEXT'] = ''\n",
    "meta_nyt_cut['TEXT'] = ''\n",
    "\n",
    "# ----------------------------------- reddit ---------------------------------- #\n",
    "\n",
    "for index, row in meta_reddit_cut.iterrows():\n",
    "    # making a new filename that includes the file's path\n",
    "    filepath_name = 'REDDIT_2008_text/' + row['FILENAME']\n",
    "    text = codecs.open(corpus_path + filepath_name, \"r\", encoding='utf8').read()\n",
    "    \n",
    "    # saving the new filename and the full text of the current reddit post\n",
    "    meta_reddit_cut.at[index, 'FILENAME'] = filepath_name\n",
    "    meta_reddit_cut.at[index, 'TEXT'] = text\n",
    "    \n",
    "# ------------------------------------- nyt ------------------------------------ #\n",
    "    \n",
    "# when trying to open files using the filenames listed the NYT csv, I found some\n",
    "# entries that didn't have corresponding files in the NYT folder;\n",
    "# i thus use this list to save the indices of rows to drop\n",
    "drop_rows = []\n",
    "    \n",
    "for index, row in meta_nyt_cut.iterrows():\n",
    "    # making a new filename that includes the file's path\n",
    "    filepath_name = 'NYT_' + row['FILENAME']\n",
    "    \n",
    "    # using a try-catch block as I ran into a FileNotFoundError while testing\n",
    "    # (there were rows in the nyt csv file with filenames that do not exist in the nyt folder)\n",
    "    try:\n",
    "        text = codecs.open(corpus_path + filepath_name, \"r\", encoding='utf8').read()\n",
    "        \n",
    "    # if the current file cannot be opened (it DNE), adds the current index to drop_rows\n",
    "    except FileNotFoundError:\n",
    "        drop_rows.append(index)\n",
    "        \n",
    "        # continues to the next nyt article\n",
    "        continue\n",
    "    \n",
    "    # saving the new filename and the full text of the current article (if its file exists)\n",
    "    meta_nyt_cut.at[index, 'FILENAME'] = filepath_name\n",
    "    meta_nyt_cut.at[index, 'TEXT'] = text\n",
    "\n",
    "# dropping the rows that do not have valid files in the nyt folder\n",
    "meta_nyt_cut = meta_nyt_cut.drop(drop_rows)\n",
    "meta_nyt_cut = meta_nyt_cut.reset_index(drop=True)\n",
    "\n",
    "# -------------------------------- concatenation ------------------------------- #\n",
    "\n",
    "# combining the metadata for reddit and nyt into one dataframe and resetting the indices\n",
    "meta_both = pd.concat([meta_reddit_cut, meta_nyt_cut])\n",
    "meta_both = meta_both.reset_index(drop=True)\n",
    "\n",
    "meta_both.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 1 contd (pt 3)\n",
    "\n",
    "import re\n",
    "\n",
    "''' making a custom pre-processor for the next cell's CountVectorizer '''\n",
    "\n",
    "# elimintes numbers and instances of \"_\", \"\\\", and \"—\"\n",
    "def my_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('([0-9—_\\\\\\\\])', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>...</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zwilling</th>\n",
       "      <th>zyuganov</th>\n",
       "      <th>álvaro</th>\n",
       "      <th>ángel</th>\n",
       "      <th>édgar</th>\n",
       "      <th>élysée</th>\n",
       "      <th>état</th>\n",
       "      <th>óscar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c035ug0.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>On your social networks all your friends and c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c039a2t.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Clearly Angela Durante is in dire need of a cu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03ad2h.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>If you enjoyed this video, you will love this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03cnx9.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Seriously doubt truthnews factors into that.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03d5p6.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I have a major issue with the work skeptic the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>NYT_2008/5/5481dcc338f0d874625c9456.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>NYT_2008/5/5481dc6a38f0d874625c944b.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoping to curb the increase in the number of y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>NYT_2008/5/5481dc6538f0d874625c944a.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>BEIRUT, Lebanon — A gunman opened fire in a mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>NYT_2008/5/5481dc5d38f0d874625c9449.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>PARIS — It is \\nVladimir V. Putin\\n’s first tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>NYT_2008/5/5481dc4038f0d874625c9444.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9979 rows × 20599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FILENAME SOURCE  \\\n",
       "0                REDDIT_2008_text/c035ug0.txt      0   \n",
       "1                REDDIT_2008_text/c039a2t.txt      0   \n",
       "2                REDDIT_2008_text/c03ad2h.txt      0   \n",
       "3                REDDIT_2008_text/c03cnx9.txt      0   \n",
       "4                REDDIT_2008_text/c03d5p6.txt      0   \n",
       "...                                       ...    ...   \n",
       "9974  NYT_2008/5/5481dcc338f0d874625c9456.txt      1   \n",
       "9975  NYT_2008/5/5481dc6a38f0d874625c944b.txt      1   \n",
       "9976  NYT_2008/5/5481dc6538f0d874625c944a.txt      1   \n",
       "9977  NYT_2008/5/5481dc5d38f0d874625c9449.txt      1   \n",
       "9978  NYT_2008/5/5481dc4038f0d874625c9444.txt      1   \n",
       "\n",
       "                                                   TEXT  aaron  aba  aback  \\\n",
       "0     On your social networks all your friends and c...      0    0      0   \n",
       "1     Clearly Angela Durante is in dire need of a cu...      0    0      0   \n",
       "2     If you enjoyed this video, you will love this ...      0    0      0   \n",
       "3         Seriously doubt truthnews factors into that.       0    0      0   \n",
       "4     I have a major issue with the work skeptic the...      0    0      0   \n",
       "...                                                 ...    ...  ...    ...   \n",
       "9974  BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...      0    0      0   \n",
       "9975  Hoping to curb the increase in the number of y...      0    0      0   \n",
       "9976  BEIRUT, Lebanon — A gunman opened fire in a mo...      0    0      0   \n",
       "9977  PARIS — It is \\nVladimir V. Putin\\n’s first tr...      0    0      0   \n",
       "9978  PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...      0    0      0   \n",
       "\n",
       "      abandon  abandoned  abandoning  abandonment  ...  zuma  zurich  \\\n",
       "0           0          0           0            0  ...     0       0   \n",
       "1           0          0           0            0  ...     0       0   \n",
       "2           0          0           0            0  ...     0       0   \n",
       "3           0          0           0            0  ...     0       0   \n",
       "4           0          0           0            0  ...     0       0   \n",
       "...       ...        ...         ...          ...  ...   ...     ...   \n",
       "9974        0          0           0            0  ...     0       0   \n",
       "9975        0          0           0            0  ...     0       0   \n",
       "9976        0          0           0            0  ...     0       0   \n",
       "9977        0          0           0            0  ...     0       0   \n",
       "9978        0          0           0            0  ...     0       0   \n",
       "\n",
       "      zwilling  zyuganov  álvaro  ángel  édgar  élysée  état  óscar  \n",
       "0            0         0       0      0      0       0     0      0  \n",
       "1            0         0       0      0      0       0     0      0  \n",
       "2            0         0       0      0      0       0     0      0  \n",
       "3            0         0       0      0      0       0     0      0  \n",
       "4            0         0       0      0      0       0     0      0  \n",
       "...        ...       ...     ...    ...    ...     ...   ...    ...  \n",
       "9974         0         0       0      0      0       0     0      0  \n",
       "9975         0         0       0      0      0       0     0      0  \n",
       "9976         0         0       0      0      0       0     0      0  \n",
       "9977         0         0       0      0      0       0     0      0  \n",
       "9978         0         0       0      0      0       0     0      0  \n",
       "\n",
       "[9979 rows x 20599 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 1 contd (pt 4)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "''' using CountVectorizer to make a DTM based on the words in the corpus '''\n",
    "\n",
    "# creating a new vecorizer\n",
    "vectorizer = CountVectorizer(input='content', preprocessor=my_preprocessor, stop_words='english', min_df=5, encoding='utf8')\n",
    "dtm = vectorizer.fit_transform(meta_both['TEXT'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "\n",
    "# combining the DTM with the metadata\n",
    "DTM = pd.DataFrame(matrix, columns=vocab)\n",
    "\n",
    "# attaching the DTM to the original dataframe\n",
    "dtm_both = pd.concat([meta_both, DTM], axis=1)\n",
    "\n",
    "# changing all instances of \"Reddit\" to 0 and \"NYT\" to 1 under the \"SOURCE\" column\n",
    "# for the following sklearn cells\n",
    "dtm_both.loc[dtm_both.SOURCE == 'Reddit', 'SOURCE'] = 0\n",
    "dtm_both.loc[dtm_both.SOURCE == 'NYT', 'SOURCE'] = 1\n",
    "\n",
    "dtm_both.head(-20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 1 contd (pt 5)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "''' preparing for the ML part of the pset; creating training and test sets '''\n",
    "\n",
    "# the x values are the words in the DTM\n",
    "x_values = dtm_both.iloc[:, 3:].values.astype(float)\n",
    "\n",
    "# the y values are the 0s and 1s representing whether a file is from reddit or nyt\n",
    "y_values = dtm_both.iloc[:, 1].values.astype(float)\n",
    "\n",
    "# splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# predicting class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# generating class probabilities\n",
    "probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of classification: 0.9763333333333334\n",
      "[[1485    6]\n",
      " [  65 1444]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      1491\n",
      "         1.0       1.00      0.96      0.98      1509\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.98      0.98      0.98      3000\n",
      "weighted avg       0.98      0.98      0.98      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 1 contd (pt 6)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "''' testing the model using the test sets and the predicted class labels '''\n",
    "\n",
    "# predicting class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# generating class probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# evaluation metrics; accuracy + ROC\n",
    "print(\"Success rate of classification: \" + str(accuracy_score(y_test, predicted)))\n",
    "\n",
    "# confusion matrix, F-1 score\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features associated with second class (NYT)\n",
      "\n",
      "('david', 11.837559025308545)\n",
      "('photo', 12.59629125194119)\n",
      "('estate', 13.840026357659116)\n",
      "('arab', 14.205758348857957)\n",
      "('photograph', 22.83632120579337)\n",
      "('grace', 23.348727039549573)\n",
      "('fighting', 24.990698888769312)\n",
      "('manhattan', 26.865768667457512)\n",
      "('nytimes', 30.16152804155062)\n",
      "('rep', 31.959556851618185)\n",
      "\n",
      "\n",
      "Top 10 features associated with first class (Reddit)\n",
      "\n",
      "('http', 0.012091556432413246)\n",
      "('gt', 0.12503050284473832)\n",
      "('actually', 0.2054899570363611)\n",
      "('just', 0.2811349424141666)\n",
      "('world', 0.33645469131491484)\n",
      "('voting', 0.3664010556900865)\n",
      "('shit', 0.4309272834377997)\n",
      "('war', 0.4496562383351771)\n",
      "('really', 0.45970140426577927)\n",
      "('article', 0.46541394413643067)\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 1 contd (pt 7)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "''' outputting the top features distinguishing files from reddit vs those from nyt '''\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "# keeping track of feature names and class labels\n",
    "feature_names = dtm_both.columns[3:].values     \n",
    "class_labels = dtm_both['SOURCE'].unique()\n",
    "\n",
    "# getting the co-efficients for the features associated with nyt and matching them to their feature name\n",
    "top20 = np.argsort(np.exp(clf.coef_))[0][-10:] \n",
    "\n",
    "# getting the co-efficients for the features associated with reddit matching them to their feature name\n",
    "bottom20 = np.argsort(np.exp(clf.coef_))[0][:10]\n",
    "\n",
    "# outputting the top 10 features associated with nyt and the top 10 associated with reddit\n",
    "print(\"Top 10 features associated with second class (NYT)\\n\")\n",
    "for el in zip(feature_names[top20], np.exp(clf.coef_)[0][top20]):\n",
    "    print(el)\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 features associated with first class (Reddit)\\n\")\n",
    "for el in zip(feature_names[bottom20], np.exp(clf.coef_)[0][bottom20]):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSee the reflections under each experiment\\'s \"top 10 features\" cell\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 pre-explanation\n",
    "\n",
    "# what i thought\n",
    "''' \n",
    "For the open-ended part of this probleem set, I thought that the differences between NYT articles and\n",
    "Reddit posts would be pretty clear, given the drastic contrast in tone that I've personally experienced\n",
    "when reading the former versus the latter. \n",
    "\n",
    "I thus decided to do two experiments:\n",
    "1) Compare the superlative \"best\" adjectives between the two corpora\n",
    "2) Compare the third-person singular verbs between the NYT articles and Reddit comments that got more downvotes than upvotes\n",
    "\n",
    "My predictions:\n",
    "1) Reddit would have a lot more negative superlative \"best\" adjectives (e.g., worst, dirtiest, grossest). Given that\n",
    "these types of words generally convey a strong stance and Reddit is known to be a forum where individuals share their (often\n",
    "strong) opinions, I thought it would make sense to see more negative superlative adjectives in Reddit posts than from copy-edited,\n",
    "more professional news articles\n",
    "2) Reddit would have more physically oriented third-person singular verbs. These verbs appear in sentences in which another person is\n",
    "being described. Since these Reddit posts are taken from the News subreddit, I think people are more inclined to write about others\n",
    "who have \"done something,\" if that makes sense, compared to news articles which I view as more descriptive\n",
    "'''\n",
    "\n",
    "# what i did\n",
    "'''\n",
    "1) Basically, I recreated the same pipeline from question 1 but with a custom pre-processor that takes as input a list\n",
    "of tuples (where the first element is the word and the second is its POS tag) and outputs the words with the \"JJS\" tag\n",
    "as a string\n",
    "2) For this experiment, I only used the Reddit posts with net negative votes. I did this by creating a new dataframe that isolated\n",
    "rows with entries < 0 in the 'score' column of the original Reddit csv file. Then, I made another custom pre-processor that\n",
    "takes as input a list of tuples (where the first element is the word and the second is its POS tag) and outputs the words with\n",
    "the \"VBZ\" tag as a string\n",
    "'''\n",
    "\n",
    "# what happened\n",
    "'''\n",
    "See the reflections under each experiment's \"top 10 features\" cell\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORD_TOKS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYT_2008/1/547c5dd838f0d813efccc063.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>DES MOINES — As Mitt Romney, Fred S. Thompson ...</td>\n",
       "      <td>[des, moines, —, as, mitt, romney, ,, fred, s....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYT_2008/1/547c5b2538f0d813efccc022.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>DES MOINES —  Dennis Kucinich today urged his ...</td>\n",
       "      <td>[des, moines, —, dennis, kucinich, today, urge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NYT_2008/1/547c58a538f0d813efccbfde.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PORT ST. LUCIE, Fla. — On Nov. 28, Marcia L. D...</td>\n",
       "      <td>[port, st., lucie, ,, fla., —, on, nov., 28, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYT_2008/1/547c584738f0d813efccbfd4.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>DES MOINES — The Democratic presidential candi...</td>\n",
       "      <td>[des, moines, —, the, democratic, presidential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYT_2008/1/547c5a9b38f0d813efccc017.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>STANLEY, N.D. — At dawn, people from faraway s...</td>\n",
       "      <td>[stanley, ,, n.d., —, at, dawn, ,, people, fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NYT_2008/1/547c57c038f0d813efccbfc8.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>DES MOINES — Spurred by a recent \\nSupreme Cou...</td>\n",
       "      <td>[des, moines, —, spurred, by, a, recent, supre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NYT_2008/1/547c579938f0d813efccbfc5.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>DES MOINES — \\nIowa\\n is packed with president...</td>\n",
       "      <td>[des, moines, —, iowa, is, packed, with, presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NYT_2008/1/547c578d38f0d813efccbfc4.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>LOS ANGELES — Sara Jane Moore, a 1970s radical...</td>\n",
       "      <td>[los, angeles, —, sara, jane, moore, ,, a, 197...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NYT_2008/1/547c575e38f0d813efccbfc0.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>DES MOINES — Just before Thanksgiving, \\nMitt ...</td>\n",
       "      <td>[des, moines, —, just, before, thanksgiving, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NYT_2008/1/547c574938f0d813efccbfbe.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>WASHINGTON — \\nPakistan\\n’s ambassador to the ...</td>\n",
       "      <td>[washington, —, pakistan, ’, s, ambassador, to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  FILENAME SOURCE  \\\n",
       "0  NYT_2008/1/547c5dd838f0d813efccc063.txt    NYT   \n",
       "1  NYT_2008/1/547c5b2538f0d813efccc022.txt    NYT   \n",
       "2  NYT_2008/1/547c58a538f0d813efccbfde.txt    NYT   \n",
       "3  NYT_2008/1/547c584738f0d813efccbfd4.txt    NYT   \n",
       "4  NYT_2008/1/547c5a9b38f0d813efccc017.txt    NYT   \n",
       "5  NYT_2008/1/547c57c038f0d813efccbfc8.txt    NYT   \n",
       "6  NYT_2008/1/547c579938f0d813efccbfc5.txt    NYT   \n",
       "7  NYT_2008/1/547c578d38f0d813efccbfc4.txt    NYT   \n",
       "8  NYT_2008/1/547c575e38f0d813efccbfc0.txt    NYT   \n",
       "9  NYT_2008/1/547c574938f0d813efccbfbe.txt    NYT   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  DES MOINES — As Mitt Romney, Fred S. Thompson ...   \n",
       "1  DES MOINES —  Dennis Kucinich today urged his ...   \n",
       "2  PORT ST. LUCIE, Fla. — On Nov. 28, Marcia L. D...   \n",
       "3  DES MOINES — The Democratic presidential candi...   \n",
       "4  STANLEY, N.D. — At dawn, people from faraway s...   \n",
       "5  DES MOINES — Spurred by a recent \\nSupreme Cou...   \n",
       "6  DES MOINES — \\nIowa\\n is packed with president...   \n",
       "7  LOS ANGELES — Sara Jane Moore, a 1970s radical...   \n",
       "8  DES MOINES — Just before Thanksgiving, \\nMitt ...   \n",
       "9  WASHINGTON — \\nPakistan\\n’s ambassador to the ...   \n",
       "\n",
       "                                           WORD_TOKS  \n",
       "0  [des, moines, —, as, mitt, romney, ,, fred, s....  \n",
       "1  [des, moines, —, dennis, kucinich, today, urge...  \n",
       "2  [port, st., lucie, ,, fla., —, on, nov., 28, ,...  \n",
       "3  [des, moines, —, the, democratic, presidential...  \n",
       "4  [stanley, ,, n.d., —, at, dawn, ,, people, fro...  \n",
       "5  [des, moines, —, spurred, by, a, recent, supre...  \n",
       "6  [des, moines, —, iowa, is, packed, with, presi...  \n",
       "7  [los, angeles, —, sara, jane, moore, ,, a, 197...  \n",
       "8  [des, moines, —, just, before, thanksgiving, ,...  \n",
       "9  [washington, —, pakistan, ’, s, ambassador, to...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "''' adding a new column to the dataframes for both corpora with their texts tokenized by words '''\n",
    "\n",
    "# keeping the two dataframes separate so that I can the meta_nyt_cut one with a different version of\n",
    "# reddit metadata later on without having to recompute WORD_TOKS and other info\n",
    "meta_reddit_cut['WORD_TOKS'] = ''\n",
    "meta_nyt_cut['WORD_TOKS'] = ''\n",
    "\n",
    "# ----------------------------------- reddit ---------------------------------- #\n",
    "\n",
    "for index, row in meta_reddit_cut.iterrows():\n",
    "    # tokenizing each text's words, then saving these to a column\n",
    "    meta_reddit_cut.at[index, 'WORD_TOKS'] = word_tokenize(row['TEXT'].lower())\n",
    "    \n",
    "# ------------------------------------- nyt ------------------------------------ #\n",
    "    \n",
    "for index, row in meta_nyt_cut.iterrows():\n",
    "    # tokenizing each text's words, then saving these to a column\n",
    "    meta_nyt_cut.at[index, 'WORD_TOKS'] = word_tokenize(row['TEXT'].lower())\n",
    "    \n",
    "meta_nyt_cut.head(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halfway there!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORD_TOKS</th>\n",
       "      <th>POS_TAGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c035ug0.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>On your social networks all your friends and c...</td>\n",
       "      <td>[on, your, social, networks, all, your, friend...</td>\n",
       "      <td>[(On, IN), (your, PRP$), (social, JJ), (networ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c039a2t.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Clearly Angela Durante is in dire need of a cu...</td>\n",
       "      <td>[clearly, angela, durante, is, in, dire, need,...</td>\n",
       "      <td>[(Clearly, RB), (Angela, NNP), (Durante, NNP),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03ad2h.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>If you enjoyed this video, you will love this ...</td>\n",
       "      <td>[if, you, enjoyed, this, video, ,, you, will, ...</td>\n",
       "      <td>[(If, IN), (you, PRP), (enjoyed, VBP), (this, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03cnx9.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Seriously doubt truthnews factors into that.</td>\n",
       "      <td>[seriously, doubt, truthnews, factors, into, t...</td>\n",
       "      <td>[(Seriously, RB), (doubt, JJ), (truthnews, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03d5p6.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>I have a major issue with the work skeptic the...</td>\n",
       "      <td>[i, have, a, major, issue, with, the, work, sk...</td>\n",
       "      <td>[(I, PRP), (have, VBP), (a, DT), (major, JJ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>NYT_2008/5/5481dcc338f0d874625c9456.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...</td>\n",
       "      <td>[baghdad, —, prime, minister, nuri, kamal, al-...</td>\n",
       "      <td>[(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>NYT_2008/5/5481dc6a38f0d874625c944b.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>Hoping to curb the increase in the number of y...</td>\n",
       "      <td>[hoping, to, curb, the, increase, in, the, num...</td>\n",
       "      <td>[(Hoping, VBG), (to, TO), (curb, VB), (the, DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>NYT_2008/5/5481dc6538f0d874625c944a.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>BEIRUT, Lebanon — A gunman opened fire in a mo...</td>\n",
       "      <td>[beirut, ,, lebanon, —, a, gunman, opened, fir...</td>\n",
       "      <td>[(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>NYT_2008/5/5481dc5d38f0d874625c9449.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PARIS — It is \\nVladimir V. Putin\\n’s first tr...</td>\n",
       "      <td>[paris, —, it, is, vladimir, v., putin, ’, s, ...</td>\n",
       "      <td>[(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>NYT_2008/5/5481dc4038f0d874625c9444.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...</td>\n",
       "      <td>[paris, —, abdul, qadeer, khan, ,, the, founde...</td>\n",
       "      <td>[(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9979 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FILENAME  SOURCE  \\\n",
       "0                REDDIT_2008_text/c035ug0.txt  Reddit   \n",
       "1                REDDIT_2008_text/c039a2t.txt  Reddit   \n",
       "2                REDDIT_2008_text/c03ad2h.txt  Reddit   \n",
       "3                REDDIT_2008_text/c03cnx9.txt  Reddit   \n",
       "4                REDDIT_2008_text/c03d5p6.txt  Reddit   \n",
       "...                                       ...     ...   \n",
       "9974  NYT_2008/5/5481dcc338f0d874625c9456.txt     NYT   \n",
       "9975  NYT_2008/5/5481dc6a38f0d874625c944b.txt     NYT   \n",
       "9976  NYT_2008/5/5481dc6538f0d874625c944a.txt     NYT   \n",
       "9977  NYT_2008/5/5481dc5d38f0d874625c9449.txt     NYT   \n",
       "9978  NYT_2008/5/5481dc4038f0d874625c9444.txt     NYT   \n",
       "\n",
       "                                                   TEXT  \\\n",
       "0     On your social networks all your friends and c...   \n",
       "1     Clearly Angela Durante is in dire need of a cu...   \n",
       "2     If you enjoyed this video, you will love this ...   \n",
       "3         Seriously doubt truthnews factors into that.    \n",
       "4     I have a major issue with the work skeptic the...   \n",
       "...                                                 ...   \n",
       "9974  BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...   \n",
       "9975  Hoping to curb the increase in the number of y...   \n",
       "9976  BEIRUT, Lebanon — A gunman opened fire in a mo...   \n",
       "9977  PARIS — It is \\nVladimir V. Putin\\n’s first tr...   \n",
       "9978  PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...   \n",
       "\n",
       "                                              WORD_TOKS  \\\n",
       "0     [on, your, social, networks, all, your, friend...   \n",
       "1     [clearly, angela, durante, is, in, dire, need,...   \n",
       "2     [if, you, enjoyed, this, video, ,, you, will, ...   \n",
       "3     [seriously, doubt, truthnews, factors, into, t...   \n",
       "4     [i, have, a, major, issue, with, the, work, sk...   \n",
       "...                                                 ...   \n",
       "9974  [baghdad, —, prime, minister, nuri, kamal, al-...   \n",
       "9975  [hoping, to, curb, the, increase, in, the, num...   \n",
       "9976  [beirut, ,, lebanon, —, a, gunman, opened, fir...   \n",
       "9977  [paris, —, it, is, vladimir, v., putin, ’, s, ...   \n",
       "9978  [paris, —, abdul, qadeer, khan, ,, the, founde...   \n",
       "\n",
       "                                               POS_TAGS  \n",
       "0     [(On, IN), (your, PRP$), (social, JJ), (networ...  \n",
       "1     [(Clearly, RB), (Angela, NNP), (Durante, NNP),...  \n",
       "2     [(If, IN), (you, PRP), (enjoyed, VBP), (this, ...  \n",
       "3     [(Seriously, RB), (doubt, JJ), (truthnews, NNS...  \n",
       "4     [(I, PRP), (have, VBP), (a, DT), (major, JJ), ...  \n",
       "...                                                 ...  \n",
       "9974  [(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...  \n",
       "9975  [(Hoping, VBG), (to, TO), (curb, VB), (the, DT...  \n",
       "9976  [(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...  \n",
       "9977  [(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...  \n",
       "9978  [(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...  \n",
       "\n",
       "[9979 rows x 5 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 2)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "''' adding a new column to the dataframes with each text's POS tags '''\n",
    "\n",
    "meta_reddit_cut['POS_TAGS'] = ''\n",
    "meta_nyt_cut['POS_TAGS'] = ''\n",
    "\n",
    "# ----------------------------------- reddit ---------------------------------- #\n",
    "\n",
    "for index, row in meta_reddit_cut.iterrows():\n",
    "    \n",
    "    # creating a list of sublists of POS tags where each sublist represents a sentence\n",
    "    list_of_sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(row['TEXT'])]\n",
    "    \n",
    "    # flattening the list of sublists into a simple list of POS tags\n",
    "    meta_reddit_cut.at[index, 'POS_TAGS'] = [POS_tuple for sublist in list_of_sentences for POS_tuple in sublist]\n",
    "    \n",
    "# the above code runs a bit slow so here is a checkpoint\n",
    "print(\"halfway there!\")\n",
    "\n",
    "# ------------------------------------- nyt ------------------------------------ #\n",
    "    \n",
    "for index, row in meta_nyt_cut.iterrows():\n",
    "    \n",
    "    # creating a list of sublists of POS tags where each sublist represents a sentence\n",
    "    list_of_sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(row['TEXT'])]\n",
    "    \n",
    "    # flattening the list of sublists into a simple list of POS tags\n",
    "    meta_nyt_cut.at[index, 'POS_TAGS'] = [POS_tuple for sublist in list_of_sentences for POS_tuple in sublist]\n",
    "\n",
    "# combining the metadata for reddit and the nyt into one dataframe;\n",
    "# also renaming the filename column and resetting the indices\n",
    "meta_both = pd.concat([meta_reddit_cut, meta_nyt_cut])\n",
    "meta_both = meta_both.reset_index(drop=True)\n",
    "\n",
    "meta_both.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2 contd (pt 3)\n",
    "\n",
    "''' creating a custom pre-processor that only extracts words tagged with \"JJS\" (superlative adjectives) '''\n",
    "\n",
    "# assumes that the input is a list of POS tags\n",
    "def JJS_preprocessor(list_of_POS_tags):\n",
    "    \n",
    "    # a list of the words to output\n",
    "    output_text_as_list = []\n",
    "    \n",
    "    for POS_tuple in list_of_POS_tags:\n",
    "        \n",
    "        # if the word in the current POS tuple has numbers in it, do not save it\n",
    "        if(re.search('[0-9]', POS_tuple[0])):\n",
    "            continue\n",
    "        \n",
    "        # if the word in the current POS tuple is a superlative adjective\n",
    "        if (POS_tuple[1] in ['JJS']):\n",
    "            \n",
    "            # appends the word to a list of words to keep\n",
    "            output_text_as_list.append(POS_tuple[0].lower())\n",
    "    \n",
    "    # returns a string with each superlative adjective separated by a space\n",
    "    return \" \".join(output_text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORD_TOKS</th>\n",
       "      <th>POS_TAGS</th>\n",
       "      <th>arrest</th>\n",
       "      <th>behest</th>\n",
       "      <th>best</th>\n",
       "      <th>biggest</th>\n",
       "      <th>bitterest</th>\n",
       "      <th>...</th>\n",
       "      <th>weakest</th>\n",
       "      <th>wealthiest</th>\n",
       "      <th>west</th>\n",
       "      <th>wettest</th>\n",
       "      <th>whitest</th>\n",
       "      <th>widest</th>\n",
       "      <th>wildest</th>\n",
       "      <th>worst</th>\n",
       "      <th>wrest</th>\n",
       "      <th>youngest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c035ug0.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>On your social networks all your friends and c...</td>\n",
       "      <td>[on, your, social, networks, all, your, friend...</td>\n",
       "      <td>[(On, IN), (your, PRP$), (social, JJ), (networ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c039a2t.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Clearly Angela Durante is in dire need of a cu...</td>\n",
       "      <td>[clearly, angela, durante, is, in, dire, need,...</td>\n",
       "      <td>[(Clearly, RB), (Angela, NNP), (Durante, NNP),...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03ad2h.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>If you enjoyed this video, you will love this ...</td>\n",
       "      <td>[if, you, enjoyed, this, video, ,, you, will, ...</td>\n",
       "      <td>[(If, IN), (you, PRP), (enjoyed, VBP), (this, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03cnx9.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Seriously doubt truthnews factors into that.</td>\n",
       "      <td>[seriously, doubt, truthnews, factors, into, t...</td>\n",
       "      <td>[(Seriously, RB), (doubt, JJ), (truthnews, NNS...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03d5p6.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>I have a major issue with the work skeptic the...</td>\n",
       "      <td>[i, have, a, major, issue, with, the, work, sk...</td>\n",
       "      <td>[(I, PRP), (have, VBP), (a, DT), (major, JJ), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>NYT_2008/5/5481dcc338f0d874625c9456.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...</td>\n",
       "      <td>[baghdad, —, prime, minister, nuri, kamal, al-...</td>\n",
       "      <td>[(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>NYT_2008/5/5481dc6a38f0d874625c944b.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoping to curb the increase in the number of y...</td>\n",
       "      <td>[hoping, to, curb, the, increase, in, the, num...</td>\n",
       "      <td>[(Hoping, VBG), (to, TO), (curb, VB), (the, DT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>NYT_2008/5/5481dc6538f0d874625c944a.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>BEIRUT, Lebanon — A gunman opened fire in a mo...</td>\n",
       "      <td>[beirut, ,, lebanon, —, a, gunman, opened, fir...</td>\n",
       "      <td>[(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>NYT_2008/5/5481dc5d38f0d874625c9449.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>PARIS — It is \\nVladimir V. Putin\\n’s first tr...</td>\n",
       "      <td>[paris, —, it, is, vladimir, v., putin, ’, s, ...</td>\n",
       "      <td>[(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>NYT_2008/5/5481dc4038f0d874625c9444.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...</td>\n",
       "      <td>[paris, —, abdul, qadeer, khan, ,, the, founde...</td>\n",
       "      <td>[(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9979 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FILENAME SOURCE  \\\n",
       "0                REDDIT_2008_text/c035ug0.txt      0   \n",
       "1                REDDIT_2008_text/c039a2t.txt      0   \n",
       "2                REDDIT_2008_text/c03ad2h.txt      0   \n",
       "3                REDDIT_2008_text/c03cnx9.txt      0   \n",
       "4                REDDIT_2008_text/c03d5p6.txt      0   \n",
       "...                                       ...    ...   \n",
       "9974  NYT_2008/5/5481dcc338f0d874625c9456.txt      1   \n",
       "9975  NYT_2008/5/5481dc6a38f0d874625c944b.txt      1   \n",
       "9976  NYT_2008/5/5481dc6538f0d874625c944a.txt      1   \n",
       "9977  NYT_2008/5/5481dc5d38f0d874625c9449.txt      1   \n",
       "9978  NYT_2008/5/5481dc4038f0d874625c9444.txt      1   \n",
       "\n",
       "                                                   TEXT  \\\n",
       "0     On your social networks all your friends and c...   \n",
       "1     Clearly Angela Durante is in dire need of a cu...   \n",
       "2     If you enjoyed this video, you will love this ...   \n",
       "3         Seriously doubt truthnews factors into that.    \n",
       "4     I have a major issue with the work skeptic the...   \n",
       "...                                                 ...   \n",
       "9974  BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...   \n",
       "9975  Hoping to curb the increase in the number of y...   \n",
       "9976  BEIRUT, Lebanon — A gunman opened fire in a mo...   \n",
       "9977  PARIS — It is \\nVladimir V. Putin\\n’s first tr...   \n",
       "9978  PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...   \n",
       "\n",
       "                                              WORD_TOKS  \\\n",
       "0     [on, your, social, networks, all, your, friend...   \n",
       "1     [clearly, angela, durante, is, in, dire, need,...   \n",
       "2     [if, you, enjoyed, this, video, ,, you, will, ...   \n",
       "3     [seriously, doubt, truthnews, factors, into, t...   \n",
       "4     [i, have, a, major, issue, with, the, work, sk...   \n",
       "...                                                 ...   \n",
       "9974  [baghdad, —, prime, minister, nuri, kamal, al-...   \n",
       "9975  [hoping, to, curb, the, increase, in, the, num...   \n",
       "9976  [beirut, ,, lebanon, —, a, gunman, opened, fir...   \n",
       "9977  [paris, —, it, is, vladimir, v., putin, ’, s, ...   \n",
       "9978  [paris, —, abdul, qadeer, khan, ,, the, founde...   \n",
       "\n",
       "                                               POS_TAGS  arrest  behest  best  \\\n",
       "0     [(On, IN), (your, PRP$), (social, JJ), (networ...       0       0     0   \n",
       "1     [(Clearly, RB), (Angela, NNP), (Durante, NNP),...       0       0     0   \n",
       "2     [(If, IN), (you, PRP), (enjoyed, VBP), (this, ...       0       0     0   \n",
       "3     [(Seriously, RB), (doubt, JJ), (truthnews, NNS...       0       0     0   \n",
       "4     [(I, PRP), (have, VBP), (a, DT), (major, JJ), ...       0       0     0   \n",
       "...                                                 ...     ...     ...   ...   \n",
       "9974  [(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...       0       0     0   \n",
       "9975  [(Hoping, VBG), (to, TO), (curb, VB), (the, DT...       0       0     0   \n",
       "9976  [(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...       0       0     0   \n",
       "9977  [(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...       0       0     0   \n",
       "9978  [(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...       0       0     0   \n",
       "\n",
       "      biggest  bitterest  ...  weakest  wealthiest  west  wettest  whitest  \\\n",
       "0           0          0  ...        0           0     0        0        0   \n",
       "1           0          0  ...        0           0     0        0        0   \n",
       "2           0          0  ...        0           0     0        0        0   \n",
       "3           0          0  ...        0           0     0        0        0   \n",
       "4           0          0  ...        0           0     0        0        0   \n",
       "...       ...        ...  ...      ...         ...   ...      ...      ...   \n",
       "9974        0          0  ...        0           0     0        0        0   \n",
       "9975        0          0  ...        0           0     0        0        0   \n",
       "9976        0          0  ...        0           0     0        0        0   \n",
       "9977        0          0  ...        0           0     0        0        0   \n",
       "9978        0          0  ...        0           0     0        0        0   \n",
       "\n",
       "      widest  wildest  worst  wrest  youngest  \n",
       "0          0        0      0      0         0  \n",
       "1          0        0      0      0         0  \n",
       "2          0        0      0      0         0  \n",
       "3          0        0      0      0         0  \n",
       "4          0        0      0      0         0  \n",
       "...      ...      ...    ...    ...       ...  \n",
       "9974       0        0      0      0         0  \n",
       "9975       0        0      0      0         0  \n",
       "9976       0        0      0      0         0  \n",
       "9977       0        0      0      0         0  \n",
       "9978       0        0      0      0         0  \n",
       "\n",
       "[9979 rows x 136 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 4)\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "''' re-doing the previous CountVectorizer step to create a new DTM with only superlative adjectives '''\n",
    "\n",
    "# creating a new vecorizer\n",
    "vectorizer = CountVectorizer(input='content', preprocessor=JJS_preprocessor, stop_words='english', min_df=2, encoding='utf8')\n",
    "dtm = vectorizer.fit_transform(meta_both['POS_TAGS'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "\n",
    "# combining the DTM with the metadata (the matrix of 0s and 1s with the vocabulary)\n",
    "DTM = pd.DataFrame(matrix, columns=vocab)\n",
    "\n",
    "# attaching the DTM to the original dataframe\n",
    "new_dtm_both = pd.concat([meta_both, DTM], axis=1)\n",
    "\n",
    "# changing all instances of \"Reddit\" to 0 and \"NYT\" to 1 under the \"SOURCE\" column\n",
    "# for the following sklearn cells\n",
    "new_dtm_both.loc[new_dtm_both.SOURCE == 'Reddit', 'SOURCE'] = 0\n",
    "new_dtm_both.loc[new_dtm_both.SOURCE == 'NYT', 'SOURCE'] = 1\n",
    "\n",
    "new_dtm_both.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of classification: 0.7286666666666667\n",
      "[[1463   42]\n",
      " [ 772  723]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.97      0.78      1505\n",
      "         1.0       0.95      0.48      0.64      1495\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.80      0.73      0.71      3000\n",
      "weighted avg       0.80      0.73      0.71      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 5)\n",
    "\n",
    "''' preparing for the ML part of the pset; creating training and test sets '''\n",
    "\n",
    "# the x values are the superlative adjectives\n",
    "x_values = new_dtm_both.iloc[:, 5:].values.astype(float)\n",
    "\n",
    "# the y values are the 0s and 1s representing whether a file is from reddit or nyt\n",
    "y_values = new_dtm_both.iloc[:, 1].values.astype(float)\n",
    "\n",
    "# splitting the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# predicting class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# generating class probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Evaluation metrics; accuracy + ROC\n",
    "print(\"Success rate of classification: \" + str(accuracy_score(y_test, predicted)))\n",
    "\n",
    "# Confusion matrix, F-1 score\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features associated with second class (NYT)\n",
      "\n",
      "('nicest', 19.08358818156899)\n",
      "('thorniest', 19.614290010682904)\n",
      "('behest', 20.417162506292197)\n",
      "('worst', 21.990658672159253)\n",
      "('weakest', 24.95521994676611)\n",
      "('cleanest', 25.206367264325735)\n",
      "('inquest', 36.39991961445652)\n",
      "('invest', 55.10351668562152)\n",
      "('stickiest', 55.34939837760005)\n",
      "('healthiest', 90.34219795115624)\n",
      "\n",
      "\n",
      "Top 10 features associated with first class (Reddit)\n",
      "\n",
      "('farthest', 0.1302119765884973)\n",
      "('saddest', 1.0)\n",
      "('safest', 1.0)\n",
      "('gravest', 1.0)\n",
      "('savviest', 1.0)\n",
      "('furthest', 1.0)\n",
      "('fullest', 1.0)\n",
      "('richest', 1.0)\n",
      "('fourth', 1.0)\n",
      "('foremost', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 6)\n",
    "\n",
    "''' outputting the top superlative adjectives distinguishing files from reddit vs those from nyt '''\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear') # penalty='l1', C=0.1\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "# keeping track of feature names and class labels\n",
    "feature_names = new_dtm_both.columns[3:].values     \n",
    "class_labels = new_dtm_both['SOURCE'].unique()\n",
    "\n",
    "# getting the co-efficients for the features associated with nyt and matching them to their feature name\n",
    "top20 = np.argsort(np.exp(clf.coef_))[0][-10:] \n",
    "\n",
    "# getting the co-efficients for the features associated with reddit matching them to their feature name\n",
    "bottom20 = np.argsort(np.exp(clf.coef_))[0][:10]\n",
    "\n",
    "# outputting the top 10 features associated with nyt and the top 10 associated with reddit\n",
    "print(\"Top 10 features associated with second class (NYT)\\n\")\n",
    "for el in zip(feature_names[top20], np.exp(clf.coef_)[0][top20]):\n",
    "    print(el)\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 features associated with first class (Reddit)\\n\")\n",
    "for el in zip(feature_names[bottom20], np.exp(clf.coef_)[0][bottom20]):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRecall: I hypothesized that Reddit would have a lot more negative superlative \"best\" adjectives (e.g., worst, dirtiest, grossest).\\nInstead, it seems that there are no distinguishing features for Reddit besides \"strictest\", which is nevertheless rather\\nclose to the \"baseline\" (or no impact) value of 1.0. Instead, it seems that it is the NYT artciles that are more heavily\\ndefined by superlative \"best\" adjectives, which is odd as I would think that these would simply describe situations rather than\\ntake a specific value judgment. For me, at least, this is pretty interesting! Perhaps a similar line of logic may bear a more\\nrigorous experiment supporting our common intuition that the news isn\\'t as impartial as it seems.\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd -- REFLECTION\n",
    "\n",
    "# What happened\n",
    "'''\n",
    "Recall: I hypothesized that Reddit would have a lot more negative superlative \"best\" adjectives (e.g., worst, dirtiest, grossest).\n",
    "Instead, it seems that there are no distinguishing features for Reddit besides \"farthest\", which is nevertheless rather\n",
    "close to the \"baseline\" (or no impact) value of 1.0. Instead, it seems that it is the NYT articles that are more heavily\n",
    "defined by superlative \"best\" adjectives, which is odd as I would think that these would simply describe situations rather than\n",
    "take a specific value judgment. For me, at least, this is pretty interesting! Perhaps a similar line of logic may bear a more\n",
    "rigorous experiment supporting our common intuition that the news isn't as impartial as it seems.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORD_TOKS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c03hiq5.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>\"She just took the kids and left. After that I...</td>\n",
       "      <td>[``, she, just, took, the, kids, and, left, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c03hjlb.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Well then do your duty and put a bullet in you...</td>\n",
       "      <td>[well, then, do, your, duty, and, put, a, bull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03is70.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Sweet.</td>\n",
       "      <td>[sweet, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03kear.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Stop acting like this girl didn't deserve it. ...</td>\n",
       "      <td>[stop, acting, like, this, girl, did, n't, des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03ppth.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>It's just underreported because most of the cr...</td>\n",
       "      <td>[it, 's, just, underreported, because, most, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>REDDIT_2008_text/c03px8l.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Not only that, we built it so god knows what k...</td>\n",
       "      <td>[not, only, that, ,, we, built, it, so, god, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>REDDIT_2008_text/c03raq0.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Still think we aren't living in a police state...</td>\n",
       "      <td>[still, think, we, are, n't, living, in, a, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>REDDIT_2008_text/c03tpbr.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Yeah, right! Consider the source: The Washingt...</td>\n",
       "      <td>[yeah, ,, right, !, consider, the, source, :, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>REDDIT_2008_text/c03wgbu.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>A very effective ban is already in place: insu...</td>\n",
       "      <td>[a, very, effective, ban, is, already, in, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REDDIT_2008_text/c03xc1s.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>I just can't believe so many Americans are hur...</td>\n",
       "      <td>[i, just, ca, n't, believe, so, many, american...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       FILENAME  SOURCE  \\\n",
       "0  REDDIT_2008_text/c03hiq5.txt  Reddit   \n",
       "1  REDDIT_2008_text/c03hjlb.txt  Reddit   \n",
       "2  REDDIT_2008_text/c03is70.txt  Reddit   \n",
       "3  REDDIT_2008_text/c03kear.txt  Reddit   \n",
       "4  REDDIT_2008_text/c03ppth.txt  Reddit   \n",
       "5  REDDIT_2008_text/c03px8l.txt  Reddit   \n",
       "6  REDDIT_2008_text/c03raq0.txt  Reddit   \n",
       "7  REDDIT_2008_text/c03tpbr.txt  Reddit   \n",
       "8  REDDIT_2008_text/c03wgbu.txt  Reddit   \n",
       "9  REDDIT_2008_text/c03xc1s.txt  Reddit   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0  \"She just took the kids and left. After that I...   \n",
       "1  Well then do your duty and put a bullet in you...   \n",
       "2                                             Sweet.   \n",
       "3  Stop acting like this girl didn't deserve it. ...   \n",
       "4  It's just underreported because most of the cr...   \n",
       "5  Not only that, we built it so god knows what k...   \n",
       "6  Still think we aren't living in a police state...   \n",
       "7  Yeah, right! Consider the source: The Washingt...   \n",
       "8  A very effective ban is already in place: insu...   \n",
       "9  I just can't believe so many Americans are hur...   \n",
       "\n",
       "                                           WORD_TOKS  \n",
       "0  [``, she, just, took, the, kids, and, left, .,...  \n",
       "1  [well, then, do, your, duty, and, put, a, bull...  \n",
       "2                                         [sweet, .]  \n",
       "3  [stop, acting, like, this, girl, did, n't, des...  \n",
       "4  [it, 's, just, underreported, because, most, o...  \n",
       "5  [not, only, that, ,, we, built, it, so, god, k...  \n",
       "6  [still, think, we, are, n't, living, in, a, po...  \n",
       "7  [yeah, ,, right, !, consider, the, source, :, ...  \n",
       "8  [a, very, effective, ban, is, already, in, pla...  \n",
       "9  [i, just, ca, n't, believe, so, many, american...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 7)\n",
    "\n",
    "''' redoing the above, but only with reddit posts that have net negative votes '''\n",
    "\n",
    "# making a new dataframe\n",
    "temp_meta_reddit = pd.read_csv(corpus_path + 'REDDIT_news_2008_meta.csv', encoding='latin1')\n",
    "\n",
    "# saving only the reddit posts with net negative votes\n",
    "meta_reddit_negative = temp_meta_reddit.loc[temp_meta_reddit['score'] < 0]\n",
    "\n",
    "# deleting all columns of the original csv file except for the filename\n",
    "meta_reddit_negative = meta_reddit_negative[['file_id']].rename(columns = {'file_id': \"FILENAME\"})\n",
    "\n",
    "# creating a new column outlining the source of the files (all from reddit)\n",
    "meta_reddit_negative['SOURCE'] = 'Reddit'\n",
    "\n",
    "# resetting the index\n",
    "meta_reddit_negative = meta_reddit_negative.reset_index(drop=True)\n",
    "\n",
    "# adding a new column to the dataframe for each file's full text and tokenized words\n",
    "meta_reddit_negative['TEXT'] = ''\n",
    "meta_reddit_negative['WORD_TOKS'] = ''\n",
    "\n",
    "for index, row in meta_reddit_negative.iterrows():\n",
    "    \n",
    "    # making a new filename that includes the file's path\n",
    "    filepath_name = 'REDDIT_2008_text/' + row['FILENAME']\n",
    "    text = codecs.open(corpus_path + filepath_name, \"r\", encoding='utf8').read()\n",
    "    \n",
    "    # saving the new filename and the full text of the current reddit post\n",
    "    meta_reddit_negative.at[index, 'FILENAME'] = filepath_name\n",
    "    meta_reddit_negative.at[index, 'TEXT'] = text\n",
    "    \n",
    "    # saving the file's text tokenized by word\n",
    "    meta_reddit_negative.at[index, 'WORD_TOKS'] = word_tokenize(row['TEXT'].lower())\n",
    "\n",
    "meta_reddit_negative.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORD_TOKS</th>\n",
       "      <th>POS_TAGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c03hiq5.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>\"She just took the kids and left. After that I...</td>\n",
       "      <td>[``, she, just, took, the, kids, and, left, .,...</td>\n",
       "      <td>[(``, ``), (She, PRP), (just, RB), (took, VBD)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c03hjlb.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Well then do your duty and put a bullet in you...</td>\n",
       "      <td>[well, then, do, your, duty, and, put, a, bull...</td>\n",
       "      <td>[(Well, RB), (then, RB), (do, VB), (your, PRP$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03is70.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Sweet.</td>\n",
       "      <td>[sweet, .]</td>\n",
       "      <td>[(Sweet, NNP), (., .)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03kear.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>Stop acting like this girl didn't deserve it. ...</td>\n",
       "      <td>[stop, acting, like, this, girl, did, n't, des...</td>\n",
       "      <td>[(Stop, NNP), (acting, VBG), (like, IN), (this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03ppth.txt</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>It's just underreported because most of the cr...</td>\n",
       "      <td>[it, 's, just, underreported, because, most, o...</td>\n",
       "      <td>[(It, PRP), ('s, VBZ), (just, RB), (underrepor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>NYT_2008/5/5481dcc338f0d874625c9456.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...</td>\n",
       "      <td>[baghdad, —, prime, minister, nuri, kamal, al-...</td>\n",
       "      <td>[(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6158</th>\n",
       "      <td>NYT_2008/5/5481dc6a38f0d874625c944b.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>Hoping to curb the increase in the number of y...</td>\n",
       "      <td>[hoping, to, curb, the, increase, in, the, num...</td>\n",
       "      <td>[(Hoping, VBG), (to, TO), (curb, VB), (the, DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6159</th>\n",
       "      <td>NYT_2008/5/5481dc6538f0d874625c944a.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>BEIRUT, Lebanon — A gunman opened fire in a mo...</td>\n",
       "      <td>[beirut, ,, lebanon, —, a, gunman, opened, fir...</td>\n",
       "      <td>[(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6160</th>\n",
       "      <td>NYT_2008/5/5481dc5d38f0d874625c9449.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PARIS — It is \\nVladimir V. Putin\\n’s first tr...</td>\n",
       "      <td>[paris, —, it, is, vladimir, v., putin, ’, s, ...</td>\n",
       "      <td>[(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>NYT_2008/5/5481dc4038f0d874625c9444.txt</td>\n",
       "      <td>NYT</td>\n",
       "      <td>PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...</td>\n",
       "      <td>[paris, —, abdul, qadeer, khan, ,, the, founde...</td>\n",
       "      <td>[(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6162 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FILENAME  SOURCE  \\\n",
       "0                REDDIT_2008_text/c03hiq5.txt  Reddit   \n",
       "1                REDDIT_2008_text/c03hjlb.txt  Reddit   \n",
       "2                REDDIT_2008_text/c03is70.txt  Reddit   \n",
       "3                REDDIT_2008_text/c03kear.txt  Reddit   \n",
       "4                REDDIT_2008_text/c03ppth.txt  Reddit   \n",
       "...                                       ...     ...   \n",
       "6157  NYT_2008/5/5481dcc338f0d874625c9456.txt     NYT   \n",
       "6158  NYT_2008/5/5481dc6a38f0d874625c944b.txt     NYT   \n",
       "6159  NYT_2008/5/5481dc6538f0d874625c944a.txt     NYT   \n",
       "6160  NYT_2008/5/5481dc5d38f0d874625c9449.txt     NYT   \n",
       "6161  NYT_2008/5/5481dc4038f0d874625c9444.txt     NYT   \n",
       "\n",
       "                                                   TEXT  \\\n",
       "0     \"She just took the kids and left. After that I...   \n",
       "1     Well then do your duty and put a bullet in you...   \n",
       "2                                                Sweet.   \n",
       "3     Stop acting like this girl didn't deserve it. ...   \n",
       "4     It's just underreported because most of the cr...   \n",
       "...                                                 ...   \n",
       "6157  BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...   \n",
       "6158  Hoping to curb the increase in the number of y...   \n",
       "6159  BEIRUT, Lebanon — A gunman opened fire in a mo...   \n",
       "6160  PARIS — It is \\nVladimir V. Putin\\n’s first tr...   \n",
       "6161  PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...   \n",
       "\n",
       "                                              WORD_TOKS  \\\n",
       "0     [``, she, just, took, the, kids, and, left, .,...   \n",
       "1     [well, then, do, your, duty, and, put, a, bull...   \n",
       "2                                            [sweet, .]   \n",
       "3     [stop, acting, like, this, girl, did, n't, des...   \n",
       "4     [it, 's, just, underreported, because, most, o...   \n",
       "...                                                 ...   \n",
       "6157  [baghdad, —, prime, minister, nuri, kamal, al-...   \n",
       "6158  [hoping, to, curb, the, increase, in, the, num...   \n",
       "6159  [beirut, ,, lebanon, —, a, gunman, opened, fir...   \n",
       "6160  [paris, —, it, is, vladimir, v., putin, ’, s, ...   \n",
       "6161  [paris, —, abdul, qadeer, khan, ,, the, founde...   \n",
       "\n",
       "                                               POS_TAGS  \n",
       "0     [(``, ``), (She, PRP), (just, RB), (took, VBD)...  \n",
       "1     [(Well, RB), (then, RB), (do, VB), (your, PRP$...  \n",
       "2                                [(Sweet, NNP), (., .)]  \n",
       "3     [(Stop, NNP), (acting, VBG), (like, IN), (this...  \n",
       "4     [(It, PRP), ('s, VBZ), (just, RB), (underrepor...  \n",
       "...                                                 ...  \n",
       "6157  [(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...  \n",
       "6158  [(Hoping, VBG), (to, TO), (curb, VB), (the, DT...  \n",
       "6159  [(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...  \n",
       "6160  [(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...  \n",
       "6161  [(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...  \n",
       "\n",
       "[6162 rows x 5 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 8)\n",
    "\n",
    "''' saving the POS tags for each reddit post with net negative votes '''\n",
    "\n",
    "meta_reddit_negative['POS_TAGS'] = ''\n",
    "\n",
    "for index, row in meta_reddit_negative.iterrows():\n",
    "    \n",
    "    # creating a list of sublists of POS tags where each sublist represents a sentence\n",
    "    list_of_sentences = [pos_tag(word_tokenize(sent)) for sent in sent_tokenize(row['TEXT'])]\n",
    "    \n",
    "    # flattening the list of sublists into a simple list of POS tags\n",
    "    meta_reddit_negative.at[index, 'POS_TAGS'] = [POS_tuple for sublist in list_of_sentences for POS_tuple in sublist]\n",
    "    \n",
    "# combining the metadata for negatively-voted reddit posts and the nyt into one dataframe\n",
    "meta_both = pd.concat([meta_reddit_negative, meta_nyt_cut])\n",
    "meta_both = meta_both.reset_index(drop=True)\n",
    "\n",
    "meta_both.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2 contd (pt 9)\n",
    "\n",
    "''' creating a custom pre-processor that only extracts words tagged with \"VBZ\" (third person singular verb) '''\n",
    "\n",
    "# assumes that the input is a list of POS tags\n",
    "def VBZ_preprocessor(list_of_POS_tags):\n",
    "    \n",
    "    # a list of the words to output\n",
    "    output_text_as_list = []\n",
    "    \n",
    "    for POS_tuple in list_of_POS_tags:\n",
    "        \n",
    "        # if the word in the current POS tuple has numbers in it, do not save it\n",
    "        if(re.search('[0-9]', POS_tuple[0])):\n",
    "            continue\n",
    "        \n",
    "        # if the word in the current POS tuple is a third person singular verb\n",
    "        if (POS_tuple[1] in ['VBZ']):\n",
    "            \n",
    "            # appends the word to a list of words to keep\n",
    "            output_text_as_list.append(POS_tuple[0].lower())\n",
    "    \n",
    "    # returns a string with each superlative adjective separated by a space\n",
    "    return \" \".join(output_text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>WORD_TOKS</th>\n",
       "      <th>POS_TAGS</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abets</th>\n",
       "      <th>ablaze</th>\n",
       "      <th>absurd</th>\n",
       "      <th>abuses</th>\n",
       "      <th>...</th>\n",
       "      <th>worries</th>\n",
       "      <th>worsens</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wounds</th>\n",
       "      <th>wraps</th>\n",
       "      <th>writes</th>\n",
       "      <th>yankees</th>\n",
       "      <th>yields</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDDIT_2008_text/c03hiq5.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>\"She just took the kids and left. After that I...</td>\n",
       "      <td>[``, she, just, took, the, kids, and, left, .,...</td>\n",
       "      <td>[(``, ``), (She, PRP), (just, RB), (took, VBD)...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDDIT_2008_text/c03hjlb.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Well then do your duty and put a bullet in you...</td>\n",
       "      <td>[well, then, do, your, duty, and, put, a, bull...</td>\n",
       "      <td>[(Well, RB), (then, RB), (do, VB), (your, PRP$...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDDIT_2008_text/c03is70.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Sweet.</td>\n",
       "      <td>[sweet, .]</td>\n",
       "      <td>[(Sweet, NNP), (., .)]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDDIT_2008_text/c03kear.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>Stop acting like this girl didn't deserve it. ...</td>\n",
       "      <td>[stop, acting, like, this, girl, did, n't, des...</td>\n",
       "      <td>[(Stop, NNP), (acting, VBG), (like, IN), (this...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDDIT_2008_text/c03ppth.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>It's just underreported because most of the cr...</td>\n",
       "      <td>[it, 's, just, underreported, because, most, o...</td>\n",
       "      <td>[(It, PRP), ('s, VBZ), (just, RB), (underrepor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>NYT_2008/5/5481dcc338f0d874625c9456.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...</td>\n",
       "      <td>[baghdad, —, prime, minister, nuri, kamal, al-...</td>\n",
       "      <td>[(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6158</th>\n",
       "      <td>NYT_2008/5/5481dc6a38f0d874625c944b.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoping to curb the increase in the number of y...</td>\n",
       "      <td>[hoping, to, curb, the, increase, in, the, num...</td>\n",
       "      <td>[(Hoping, VBG), (to, TO), (curb, VB), (the, DT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6159</th>\n",
       "      <td>NYT_2008/5/5481dc6538f0d874625c944a.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>BEIRUT, Lebanon — A gunman opened fire in a mo...</td>\n",
       "      <td>[beirut, ,, lebanon, —, a, gunman, opened, fir...</td>\n",
       "      <td>[(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6160</th>\n",
       "      <td>NYT_2008/5/5481dc5d38f0d874625c9449.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>PARIS — It is \\nVladimir V. Putin\\n’s first tr...</td>\n",
       "      <td>[paris, —, it, is, vladimir, v., putin, ’, s, ...</td>\n",
       "      <td>[(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>NYT_2008/5/5481dc4038f0d874625c9444.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...</td>\n",
       "      <td>[paris, —, abdul, qadeer, khan, ,, the, founde...</td>\n",
       "      <td>[(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6162 rows × 1132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     FILENAME SOURCE  \\\n",
       "0                REDDIT_2008_text/c03hiq5.txt      0   \n",
       "1                REDDIT_2008_text/c03hjlb.txt      0   \n",
       "2                REDDIT_2008_text/c03is70.txt      0   \n",
       "3                REDDIT_2008_text/c03kear.txt      0   \n",
       "4                REDDIT_2008_text/c03ppth.txt      0   \n",
       "...                                       ...    ...   \n",
       "6157  NYT_2008/5/5481dcc338f0d874625c9456.txt      1   \n",
       "6158  NYT_2008/5/5481dc6a38f0d874625c944b.txt      1   \n",
       "6159  NYT_2008/5/5481dc6538f0d874625c944a.txt      1   \n",
       "6160  NYT_2008/5/5481dc5d38f0d874625c9449.txt      1   \n",
       "6161  NYT_2008/5/5481dc4038f0d874625c9444.txt      1   \n",
       "\n",
       "                                                   TEXT  \\\n",
       "0     \"She just took the kids and left. After that I...   \n",
       "1     Well then do your duty and put a bullet in you...   \n",
       "2                                                Sweet.   \n",
       "3     Stop acting like this girl didn't deserve it. ...   \n",
       "4     It's just underreported because most of the cr...   \n",
       "...                                                 ...   \n",
       "6157  BAGHDAD — Prime Minister \\nNuri Kamal al-Malik...   \n",
       "6158  Hoping to curb the increase in the number of y...   \n",
       "6159  BEIRUT, Lebanon — A gunman opened fire in a mo...   \n",
       "6160  PARIS — It is \\nVladimir V. Putin\\n’s first tr...   \n",
       "6161  PARIS — \\nAbdul Qadeer Khan\\n, the founder of ...   \n",
       "\n",
       "                                              WORD_TOKS  \\\n",
       "0     [``, she, just, took, the, kids, and, left, .,...   \n",
       "1     [well, then, do, your, duty, and, put, a, bull...   \n",
       "2                                            [sweet, .]   \n",
       "3     [stop, acting, like, this, girl, did, n't, des...   \n",
       "4     [it, 's, just, underreported, because, most, o...   \n",
       "...                                                 ...   \n",
       "6157  [baghdad, —, prime, minister, nuri, kamal, al-...   \n",
       "6158  [hoping, to, curb, the, increase, in, the, num...   \n",
       "6159  [beirut, ,, lebanon, —, a, gunman, opened, fir...   \n",
       "6160  [paris, —, it, is, vladimir, v., putin, ’, s, ...   \n",
       "6161  [paris, —, abdul, qadeer, khan, ,, the, founde...   \n",
       "\n",
       "                                               POS_TAGS  abandons  abets  \\\n",
       "0     [(``, ``), (She, PRP), (just, RB), (took, VBD)...         0      0   \n",
       "1     [(Well, RB), (then, RB), (do, VB), (your, PRP$...         0      0   \n",
       "2                                [(Sweet, NNP), (., .)]         0      0   \n",
       "3     [(Stop, NNP), (acting, VBG), (like, IN), (this...         0      0   \n",
       "4     [(It, PRP), ('s, VBZ), (just, RB), (underrepor...         0      0   \n",
       "...                                                 ...       ...    ...   \n",
       "6157  [(BAGHDAD, NNP), (—, NNP), (Prime, NNP), (Mini...         0      0   \n",
       "6158  [(Hoping, VBG), (to, TO), (curb, VB), (the, DT...         0      0   \n",
       "6159  [(BEIRUT, NNP), (,, ,), (Lebanon, NNP), (—, VB...         0      0   \n",
       "6160  [(PARIS, NNP), (—, NN), (It, PRP), (is, VBZ), ...         0      0   \n",
       "6161  [(PARIS, NNP), (—, NNP), (Abdul, NNP), (Qadeer...         0      0   \n",
       "\n",
       "      ablaze  absurd  abuses  ...  worries  worsens  worth  wouldn  wounds  \\\n",
       "0          0       0       0  ...        0        0      0       0       0   \n",
       "1          0       0       0  ...        0        0      0       0       0   \n",
       "2          0       0       0  ...        0        0      0       0       0   \n",
       "3          0       0       0  ...        0        0      0       0       0   \n",
       "4          0       0       0  ...        0        0      0       0       0   \n",
       "...      ...     ...     ...  ...      ...      ...    ...     ...     ...   \n",
       "6157       0       0       0  ...        0        0      0       0       0   \n",
       "6158       0       0       0  ...        0        0      0       0       0   \n",
       "6159       0       0       0  ...        0        0      0       0       0   \n",
       "6160       0       0       0  ...        0        0      0       0       0   \n",
       "6161       0       0       0  ...        0        0      0       0       0   \n",
       "\n",
       "      wraps  writes  yankees  yields  zones  \n",
       "0         0       0        0       0      0  \n",
       "1         0       0        0       0      0  \n",
       "2         0       0        0       0      0  \n",
       "3         0       0        0       0      0  \n",
       "4         0       0        0       0      0  \n",
       "...     ...     ...      ...     ...    ...  \n",
       "6157      0       0        0       0      0  \n",
       "6158      0       0        0       0      0  \n",
       "6159      0       0        0       0      0  \n",
       "6160      0       0        0       0      0  \n",
       "6161      0       0        0       0      0  \n",
       "\n",
       "[6162 rows x 1132 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 10)\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "''' re-doing the previous CountVectorizer step to create a new DTM with only third person singular verbs '''\n",
    "\n",
    "# creating a new vecorizer\n",
    "vectorizer = CountVectorizer(input='content', preprocessor=VBZ_preprocessor, stop_words='english', min_df=2, encoding='utf8')\n",
    "dtm = vectorizer.fit_transform(meta_both['POS_TAGS'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "\n",
    "# combining the DTM with the metadata (the matrix of 0s and 1s with the vocabulary)\n",
    "DTM = pd.DataFrame(matrix, columns=vocab)\n",
    "\n",
    "# attaching the DTM to the original dataframe\n",
    "new_dtm_both = pd.concat([meta_both, DTM], axis=1)\n",
    "\n",
    "# changing all instances of \"Reddit\" to 0 and \"NYT\" to 1 under the \"SOURCE\" column\n",
    "# for the following sklearn cells\n",
    "new_dtm_both.loc[new_dtm_both.SOURCE == 'Reddit', 'SOURCE'] = 0\n",
    "new_dtm_both.loc[new_dtm_both.SOURCE == 'NYT', 'SOURCE'] = 1\n",
    "\n",
    "new_dtm_both.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of classification: 0.8075471698113208\n",
      "[[   7  350]\n",
      " [   7 1491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.02      0.04       357\n",
      "         1.0       0.81      1.00      0.89      1498\n",
      "\n",
      "    accuracy                           0.81      1855\n",
      "   macro avg       0.65      0.51      0.47      1855\n",
      "weighted avg       0.75      0.81      0.73      1855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 11)\n",
    "\n",
    "''' preparing for the ML part of the pset; creating training and test sets '''\n",
    "\n",
    "# the x values are the words in the DTM\n",
    "x_values = new_dtm_both.iloc[:, 5:].values.astype(float)\n",
    "\n",
    "# the y values are the words in the DTM\n",
    "y_values = new_dtm_both.iloc[:, 1].values.astype(float)\n",
    "\n",
    "# splitting the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# predicting class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# generating class probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Evaluation metrics; accuracy + ROC\n",
    "print(\"Success rate of classification: \" + str(accuracy_score(y_test, predicted)))\n",
    "\n",
    "# Confusion matrix, F-1 score\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features associated with second class (NYT)\n",
      "\n",
      "('protests', 9.204333176713419)\n",
      "('focuses', 9.224969448927013)\n",
      "('allies', 9.929720141541189)\n",
      "('imposes', 10.529795803295354)\n",
      "('hints', 11.122221743701092)\n",
      "('anymore', 12.43151833818894)\n",
      "('represents', 12.588504090219852)\n",
      "('listens', 13.42851850050355)\n",
      "('extremists', 29.123866625380735)\n",
      "('relies', 32.32691381392926)\n",
      "\n",
      "\n",
      "Top 10 features associated with first class (Reddit)\n",
      "\n",
      "('subjects', 0.1382172079094966)\n",
      "('harms', 0.3641719169289015)\n",
      "('tastes', 0.4355412994374665)\n",
      "('crops', 0.6072723745031644)\n",
      "('urges', 0.6893698615671414)\n",
      "('soars', 0.6962809281120079)\n",
      "('gazes', 0.926521180101566)\n",
      "('preaches', 1.0)\n",
      "('precedes', 1.0)\n",
      "('praises', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2 contd (pt 12)\n",
    "\n",
    "''' outputting the top third person singular verbs distinguishing negatively-voted reddit posts vs nyt articles '''\n",
    "\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear') # penalty='l1', C=0.1\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "# keeping track of feature names and class labels\n",
    "feature_names = new_dtm_both.columns[3:].values     \n",
    "class_labels = new_dtm_both['SOURCE'].unique()\n",
    "\n",
    "# getting the co-efficients for the features associated with nyt and matching them to their feature name\n",
    "top20 = np.argsort(np.exp(clf.coef_))[0][-10:] \n",
    "\n",
    "# getting the co-efficients for the features associated with reddit matching them to their feature name\n",
    "bottom20 = np.argsort(np.exp(clf.coef_))[0][:10]\n",
    "\n",
    "# outputting the top 10 features associated with nyt and the top 10 associated with reddit\n",
    "print(\"Top 10 features associated with second class (NYT)\\n\")\n",
    "for el in zip(feature_names[top20], np.exp(clf.coef_)[0][top20]):\n",
    "    print(el)\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 features associated with first class (Reddit)\\n\")\n",
    "for el in zip(feature_names[bottom20], np.exp(clf.coef_)[0][bottom20]):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRecall: I hypothesized that  Reddit would have more physically oriented third-person singular verbs. Indeed, it seems that I \\nwas correct, in part. In the NYT, I\\'d venture that the only objectively physical verb is \"listens\" and perhaps \"focuses\", \\nwith the rest of the features being relatively abstract (\"contrasts\", \"hints\", \"relies\"). Now, compare these with \\n\"tastes\", \"gazes\", \"urges\", and \"reflects\" from Reddit -- all verbs that seem to denote a certain extent of physical embodiment.\\nThat being said, these were not the kinds of physically oriented verbs I expected! Since these features are distinguishing the\\nnegatively voted Reddit posts, I envisioned stronger verbs like \"ruins\",\"destroys\", \"fights\" -- verbs that convey a staunch\\npolitical opinion that might get one downvoted. Instead, here we have some rather reflective and seemingly temperate verbs, which\\nmakes me re-evaluate my conception of posts that are unpopular with the general public.\\n'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROBLEM 2 contd -- REFLECTION\n",
    "\n",
    "# What happened\n",
    "'''\n",
    "Recall: I hypothesized that  Reddit would have more physically oriented third-person singular verbs. Indeed, it seems that I \n",
    "was correct, in part. In the NYT, I'd venture that the only objectively physical verb is \"listens\" and perhaps \"focuses\", \n",
    "with the rest of the features being relatively abstract (\"oversees\", \"treasures\", \"relies\"). Now, compare these with \n",
    "\"harms\", \"tastes\", \"urges\", and \"gazes\" from Reddit -- all verbs that seem to denote a certain extent of physical embodiment.\n",
    "That being said, these were not the kinds of physically oriented verbs I expected! Since these features are distinguishing the\n",
    "negatively voted Reddit posts, I envisioned stronger verbs like \"ruins\",\"destroys\", \"fights\" -- verbs that convey a staunch\n",
    "political opinion that might get one downvoted. Instead, here we have some rather reflective and seemingly temperate verbs\n",
    "(with the exception of \"harms\"), which makes me re-evaluate my conception of posts that are unpopular with the general public.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
